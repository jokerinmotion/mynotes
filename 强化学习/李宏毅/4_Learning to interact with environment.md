# 	Learning to interact with environment

![image-20211207154025948](images/image-20211207154025948.png)

Actor, Environment, Rewardä¸‰è€…çš„å…³ç³»ï¼š

<img src="images/image-20211207162544526.png" alt="image-20211207162544526" style="zoom:50%;" />

## Critic

### çŠ¶æ€ä»·å€¼å‡½æ•°V^Ï€^(s)

![image-20211208200614223](images/image-20211208200614223.png)

æ³¨æ„åˆ°ï¼Œä»¥ä¸Šæ˜¯criticçš„ä¸€ç§ï¼Œ**ä¼šéšç€actorçš„ä¸åŒï¼ˆä¹Ÿå°±æ˜¯Ï€ï¼‰ï¼ŒCriticsç»™å‡ºçš„è¯„ä»·ä¹Ÿä¸åŒ**

å¦‚ä½•estimate è¿™ä¸ªV^Ï€^(s)ï¼š

- Monte-Carbo based approachï¼ˆè’™ç‰¹å¡ç½—ï¼‰

<img src="images/image-20211208205252226.png" alt="image-20211208205252226" style="zoom: 67%;" />

- Temporal-difference approachï¼ˆæ—¶åºå·®åˆ†ï¼‰

<img src="images/image-20211208205428046.png" alt="image-20211208205428046" style="zoom:67%;" />

- ä¸¤è€…åŒºåˆ«

æš‚ç•¥

### åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ˆQ^Ï€^(s,a)ï¼ŒQå‡½æ•°ï¼‰

![image-20211208211626335](images/image-20211208211626335.png)

å¯¹äºç¦»æ•£çš„åŠ¨ä½œï¼Œå¯ä»¥æ”¹å†™Qå‡½æ•°ï¼š

![image-20211208212307707](images/image-20211208212307707.png)

ğŸ’¡ä½¿ç”¨Qå‡½æ•°è¿›è¡Œç­–ç•¥è¿­ä»£ï¼ˆQ-Learningï¼‰

![image-20211208213119386](images/image-20211208213119386.png)

åŸç†ï¼š

![image-20211208213438319](images/image-20211208213438319.png)

æ³¨æ„ï¼šè¿™é‡Œçš„actionåªèƒ½æ˜¯ç¦»æ•£çš„ï¼ˆå·¦ç§»ã€å³ç§»ã€å¼€ç«ï¼‰

ğŸ’¡DQNçš„ä¸ƒç§å®ç°å’Œæ”¹è¿›ï¼š**è®ºæ–‡-Rainbow**



## Actor-Critic

> ä¹‹å‰å­¦ä¹ actorçš„æ—¶å€™æ˜¯çœ‹reward functionçš„è¾“å‡ºæ¥çœ‹å¦‚ä½•update actorï¼Œåœ¨äº’åŠ¨è¿‡ç¨‹ä¸­æœ‰éå¸¸å¤§çš„éšæœºæ€§ï¼ŒACçš„ç²¾ç¥åœ¨äºä¸å»çœ‹ç¯å¢ƒçš„rewardäº†ï¼Œå› ä¸ºç¯å¢ƒçš„rawardå˜åŒ–å¤ªå¤§ï¼Œè€Œæ˜¯è·ŸCriticå­¦ã€‚



### ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•

advantage ACï¼ˆA2Cï¼‰

<img src="images/image-20211208220652808.png" alt="image-20211208220652808" style="zoom:50%;" />



### å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•(A3C)

<img src="images/image-20211208223525632.png" alt="image-20211208223525632" style="zoom:67%;" />

### ç‰¹ä¾‹ï¼šè·¯å¾„è¡ç”Ÿç­–ç•¥æ¢¯åº¦

**pathwise derivative policy gradient**ï¼Œè·¯å¾„è¡ç”Ÿç­–ç•¥æ¢¯åº¦ï¼Œçœ‹æˆDQNè§£è¿ç»­åŠ¨ä½œçš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯ä¸€ç§ç‰¹æ®Šçš„ACæ–¹æ³•ã€‚

ä¸€èˆ¬Q-Learningåªèƒ½å¤„ç†ç¦»æ•£çš„åŠ¨ä½œï¼Œå¦‚æœæ˜¯è¿ç»­çš„ï¼Œé‚£å°±è¦è®­ç»ƒactor Ï€ï¼Œå®ƒè¾“å‡ºçš„actionæ˜¯èƒ½è®©Qå‡½æ•°çš„å€¼æœ€å¤§ã€‚åŸç†å›¾å¦‚ä¸‹ï¼š

![image-20211208224421833](images/image-20211208224421833.png)

![image-20211208224440397](images/image-20211208224440397.png)

![image-20211208230158358](images/image-20211208230158358.png)

![image-20211208225532974](images/image-20211208225532974.png)

## Inverse RL

æ²¡æœ‰reward functionã€‚å› ä¸ºç°å®ä¸­çš„å¾ˆå¤šé—®é¢˜å°±æ˜¯ä¸å¥½å®šä¹‰rewardçš„ã€‚

ç”¨ Inverse RLæ¨å‡ºreward functionï¼Œç„¶åå†ç”¨RLå»æ‰¾æœ€å¥½çš„actor

![image-20211208232137023](images/image-20211208232137023.png)

é€†å¼ºåŒ–å­¦ä¹ çš„åŸç†æ¡†æ¶ï¼š

![image-20211208232752330](images/image-20211208232752330.png)

![image-20211208232858510](images/image-20211208232858510.png)

