# 入门介绍

![image-20211130211503152](images/image-20211130211503152.png)

Policy based approach

目的：learning an Actor(也叫policy就是一个function，一般称为$π$​​​)

$π$：Action = $π$​（Observation），如下图

<img src="images/image-20211130212936850.png" alt="image-20211130212936850" style="zoom: 67%;" />

步骤：

- Step 1: define a set of function——用Neural network

输入：observation as a vector or matrix

输出：每个action对应一个**输出神经元**，一般表示为几率

![image-20211130214431170](images/image-20211130214431170.png)

比起传统的表格法，神经网络更能**泛化**

- Step 2: goodness of function 

和监督学习一样，使用代价函数，这里是使用R~θ~的期望值

![image-20211130215538745](images/image-20211130215538745.png)

每个回合表示为一个trajectory（序列）τ

![image-20211130220919970](images/image-20211130220919970.png)

**某个**τ能得到的奖励是

![image-20211130222035679](images/image-20211130222035679.png)

某个τ出现的概率表示为

![image-20211130220534696](images/image-20211130220534696.png)

因此，所有可能的τ产生的奖励就是奖励的期望值：

![image-20211130222139852](images/image-20211130222139852.png)

采样使用大数定律：

<img src="images/image-20211130222459864.png" alt="image-20211130222459864" style="zoom:67%;" />

- Step 3: pick the best function

使用gradient ascent（和descent不同，这里是要最大化目标函数R），具体步骤：

<img src="images/image-20211130222846280.png" alt="image-20211130222846280" style="zoom:67%;" />

具体的，如何计算R的微分：

<img src="images/image-20211130223311753.png" alt="image-20211130223311753" style="zoom:67%;" />

其中：

<img src="images/image-20211130223541756.png" alt="image-20211130223541756" style="zoom:67%;" />

然后取log:

<img src="images/image-20211130223627296.png" alt="image-20211130223627296" style="zoom:67%;" />

忽略与参数θ无关的项，求微分:

<img src="images/image-20211130225121044.png" alt="image-20211130225121044" style="zoom:67%;" />

最后的奖励期望值表示为：

![image-20211130225645530](images/image-20211130225645530.png)

上式需要注意的地方：

- 它是符合直觉的，它考虑的是整个trajectory的reward，而不是但一个![image-20211130230634889](images/image-20211130230634889.png)

- 加log是为了归一化

<img src="images/image-20211130231458329.png" alt="image-20211130231458329" style="zoom:67%;" />

- 一般可以加一个baseline，以应对每个R(τ)都是正的情况：

<img src="images/image-20211130231949063.png" alt="image-20211130231949063" style="zoom: 50%;" />

<img src="images/image-20211130232044500.png" alt="image-20211130232044500" style="zoom:67%;" />

> “得到的奖励超过了某个baseline，才将这个动作出现的概率增加”

